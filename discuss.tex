\section{Discussion}
\label{sec:discuss}

We conclude by briefly discussing several future research directions.
One natural question is whether we can weaken the assumptions used to construct our
succinct distributed arguments: for example, in Section~\ref{sec:distprover}
we required SNARKs to ensure consistency between the messages sent by different nodes.
However, the computation itself is deterministic, so conceivably SNARGs could suffice
(as in Section~\ref{sec:dargsForP}).
It is interesting to ask whether certifying a deterministic execution of several (or even two)
Turing machines that interact with one another runs up against the same barrier presented in~\cite{gentry2011separating}
on constructing SNARKs from standard cryptographic assumptions.

Another potential direction for obtaining better constructions is to consider weaker adversary models:
for example, we can require soundness only against an adversary that is itself an efficient \emph{distributed}
algorithm (our constructions are sound against centralized provers).
The motivation for distributed certification is typically not that the prover is \emph{malicious},
but rather that it is buggy, or the network is prone to changes.
This can be used to define weaker adversary models that would require less (or no) use of heavyweight
cryptography.

Finally, our characterization of the power of $\LDP$ algorithms
is at present incomplete.
One-way functions are a specific hardness assumption,
and an average-case one (the function is hard to invert on random inputs).
One can define a worst-case version of one-way functions,
but still, it would amount to assuming
that one \emph{specific} problem (inverting a function) is hard to solve,
and it is not clear that separating $\LDnP$ from $\LDn \cap \PP$
requires such an assumption.
It is interesting to investigate whether 
we can separate $\LDnP$ from $\LDn \cap \PP$
under a more general assumption, e.g., $\PP \neq \NP$, or $\PP \neq \NP \cap \coNP$.

