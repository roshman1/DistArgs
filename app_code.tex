\section{Pseudocode and Correctness Proofs}
\label{app:code}

\subsection{Succinct Distributed Arguments for NP from SNARKS}\label{app:dargsForNP}
\paragraph{Detailed construction.}
Let $\Lan$ be an $\NP$-language on graphs, and let $V_\Lan$ be a polynomial-time Turing machine such that:
\begin{gather*}
	G\in \Lan \Leftrightarrow \exists w \in \set{0,1}^{\poly(|G|)} \text{ such that } V_\Lan(\var{L}(\var{G}),w)=1.
\end{gather*}


Fix a vector commitment $(\VCGen, \VCCom, \VC.\Open, \VC.\Ver)$.
We define the following $\NP$ language.\footnote{We give here $1^n$ as part of the input since for a succinct commitment scheme, $c$ should be much shorter than $1^n$, whereas the witness size is bounded from below by $n^2$.}\footnote{$\aux$ is existentially quantified, but a polynomial-time verifying machine for $\Lancom$ would not need $\aux$ as part of the witness, since $\VCCom$ is polynomial-time.}
\begin{gather*}
    \Lancom =
    \left\{
    (c, 1^n, \crs)
    \middle\vert
    \ \exists L, w\ :
    \begin{array}{ll}
	    & L = (L_1,\ldots,L_m) \text{ is an adjacency list}\\
         % \qquad L_i=(v,N(v)_i) \\
         & \exists \aux: \\
         & \VCCom(\crs, L_1,\ldots, L_m) = (c, \aux) \\
         & \wedge\ V_\Lan(L, w) = 1 \\
	 & \wedge \text{the graph represented by $L$ is symmetric and connected}
    \end{array}
    \right\}
\end{gather*}

Now fix a SNARK  $(\SNARKGen, \SNARKPro, \SNARKVer, \SNARKExt)$ for $\Lancom$.
The succinct distributed argument $(\Gen, \Pro, \Ver)$ for $\Lan$ is defined as follows.




\begin{subfigures}\label{fig:dargsForNP}
    \begin{nicefig}[h]{The Setup Procedure of Section~\ref{sec:dargsForNP}: $\Gen(1^\secpar, n)$ }{fig:dargsForNPGen}
        \begin{algorithmic}[1]
            \State $\crsvc \leftarrow \VCGen(1^\secpar, n)$
            \State $\crssnark \leftarrow \SNARKGen(1^\secpar, 1^n)$
            \State Output: $\crs = (\crsvc, \crssnark)$
        \end{algorithmic}
    \end{nicefig}
    
    \begin{nicefig}[h]{The Prover of Section~\ref{sec:dargsForNP} : $\Pro(\crs, G, w)$ }{fig:dargsForNPPro}
        \begin{algorithmic}[1]
            \State Parse $\crs=(\crsvc, \crssnark)$
	    \State Let $v_1,\ldots,v_n$ be the nodes of $G$, sorted by UID
	    \State Let $L \leftarrow (L_1,\ldots,L_n)$,
	    where $L_i = ( v_i, N(v_i))$ for each $i \in [n]$
            \State $(c,\aux) \leftarrow \VCCom(\crsvc, L_1,\ldots, L_n)$
            \State Compute for every $i$: $\Lambda_i \leftarrow \VCOpen(\crsvc, L_i, i, \aux)$
            \State Compute $\pisnark \leftarrow \SNARKPro(\crssnark, c, (L, w))$
            \State Output $\{\pi(v_i)\}_{v_i\in V(G)}$, where for every $v_i\in V(G)$: $\pi(v_i) = (c, i, \Lambda_{i}, \pisnark)$
        \end{algorithmic}
    \end{nicefig}

    \begin{nicefig}[h]{The Verifier of Section~\ref{sec:dargsForNP} : $\Ver(\crs, \var{N}(v), v, \pi(v))$ }{fig:dargsForNPVer}
        \begin{algorithmic}[1]
            \State Parse $\crs=(\crsvc, \crssnark)$
            \State Parse $\pi = (c, i, \Lambda_i, \pisnark)$
            \State Verify that for every neighbor $u \in N(v)$:
	    $c(u) = c$ and $\pisnark(u) = \pisnark$ (otherwise output 0)\label{step:verifyCom}
            \State Output 1 if the following holds:
            \begin{itemize}
                \item $\VCVer(\crsvc, c, (v,\var{N}(v)), i, \Lambda_i) = 1$
                \item $\SNARKVer(\crssnark, c, \pi) = 1$
            \end{itemize}
        \end{algorithmic}
    \end{nicefig}
\end{subfigures}




\begin{claim}
$\Gen, \Pro, \Ver$ is a succinct distributed argument.
\end{claim}

\begin{proof}
Perfect completeness and succinctness follow immediately from the perfect completeness and the succinctness of the VC and the SNARK. We now prove soundness.

Assume towards contradiction that there exists an efficient prover $\Pro^*$ and a non-negligible function $\alpha(\cdot)$, such that the following holds with probability at least $\alpha(\lambda)$.
\begin{gather}\label{eq:soundnessBreakNP}
    \prob{}
    {
    \begin{array}{ll}
    G\notin \Lan \\
    \wedge\ \forall v\in V(G) : \Ver(\crs, \var{N}_G(v), v, \pi_{v}) = 1
    \end{array}
    \middle\vert
    \begin{array}{ll}
    \crs \leftarrow \Gen(1^\secpar, n) \\
    (G, \{\pi_{v}\}_{v\in V(G)}) \leftarrow \Pro^*(\crs, 1^\secpar, 1^n) \\
    \end{array}
    }
    .
\end{gather}

First, note that since all nodes verify that they agree with their neighbors
on the vector commitment $c$ and SNARK proof; if all nodes accept, then the prover gave the same 
values to all nodes. We assume this from now on.

We use $\Pro^*$ to construct an efficient adversary $\Adv$ that breaks either the SNARK or the VC.
The adversary $\Adv$ proceeds as follows: 
\begin{itemize}
	\item Given $\crs, 1^{\lambda}, n$,
		it first uses $\Pro^*(\crs, 1^{\lambda}, n)$
		to obtain a graph $G$ and certificates $\set{ \pi(v) }_{v \in V(G)}$.
	\item 
		From $\pi(v)$ (for an arbitrary $v$, since they all agree),
		the adversary extracts the vector commitment $c$ and SNARK proof $\pisnark$.
	\item The adversary extracts the $\NP$-witness $(L^*, w) \leftarrow \SNARKExt_{\Pro^*}(\crs, c, \pisnark)$
		from the SNARK proof.
	\item If $(L^*, w)$ is not a valid witness for the membership $(c, 1^n, \crs) \in \Lancom$,
		then the adversary has broken adaptive proof of knowledge property,
		as $\SNARKVer(\crssnark, c, \pi) = 1$.
	\item Otherwise, $L^*$ is an adjacency list, $L = (L_1,\ldots,L_m)$ (for some $m$ which is not necessarily
		equal to $n$), such that $\VCCom(\crs, L_1,\ldots, L_m) = (c, \aux)$,
		$V_\Lan(L, w) = 1$,
		and the graph $G'$ represented by $L$ is symmetric and connected.
		In particular, since $v_{\Lan}(L, w) = 1$,
		we have $G' \in \calL$.
		Thus, whenever $G \not \in \calL$,
		we must have $G' \neq G$, in other words, $L$
		is not the adjacency list of $G$.

		For each $v \in V$,
		let $i(v)$ be the index appearing in $v$'s certificate, $\pi(v) = (c, i(v), \Lambda(v), \pisnark)$.
		There are two cases:
		\begin{itemize}
			\item For some node $v \in V(G)$ we have
				$L_{i(v)} \neq (v, N(v))$.
				But node $v$ verifies that
				entry $i(v)$ of $c$ opens
				to its true neighborhood,
				i.e.,
				$\VCVer(\crsvc, c, (v,\var{N}(v)), i(v), \Lambda) = 1$,
				and so the adversary has broken the binding property of the vector commitment.
			\item For every $v \in V(G)$
				we have $L_{i(v)} = (v, N(v))$.
				This implies that $i(v) \neq i(v')$ for every $v \neq v' \in V(G)$,
				and therefore $\left| \set{ i(v) }_{v \in V(G)} \right| = n$.
				We claim that in this case $G = G'$,
				that is, $L$ is the true adjacency list of $G$, contradicting
				our assumption that it is not.

				If $|L| = |V(G)|$,
				then $\left| \set{ i(v) }_{v \in V(G)} \right| = n$
				and $L_{i(v)} = (v, N(v))$ for every $v \in V$,
				the list does match $G$.
				Thus, assume that $|L| \neq |V(G)|$.

				It must be that $|L| > |V(G)|$,
				as we already said that $\left| \set{ i(v) }_{v \in V(G)} \right| = n$.
				Thus, there is some entry $(u, N)$ in $L$,
				such that $u$ is not a node of $G$.
				Since $G'$ is connected,
				there is an edge $\set{w, w'}$
				in the cut between $V(G') \setminus V(G)$
				and $V(G)$,
				with $w \in V(G)$ and $w' \not \in V(G)$.
				We know that $L_{i(w)} = ( w, N(w) )$,
				but $w' \not \in N(w)$ (since $w' \not \in V(G)$);
				this is a contradiction,
				since we assumed that $L$ represents $G'$.
		\end{itemize}
\end{itemize}
\end{proof}


%Let $M$ be the polynomial Turing machine such that 
%\begin{equation*}
%(c, 1^\secpar, \crs)\in \Lancom \Leftrightarrow \exists G, w:\ M(c, 1^\secpar, \crs, L, w)=1
%.
%\end{equation*}
%
%Let $G, \{\pi_{v}\}_{v\in V(G)}$ be the graph and the proofs outputted from $\Pro^*(\crs, 1^\lambda ,n)$.
%On input $\crs, 1^\lambda, n$, $\Adv$ outputs $c, i, L_i, \Lambda_{i}, L'_i, \Lambda_{i}', \pisnark^*, L^*$, where:
%\begin{itemize}
    %\item $c, \pisnark$ are the commitment and the SNARK proof in $\pi_v$ that is consistent across all $v\in V(G)$.
    %\item $L^*$ is obtained from $\SNARKExt_{\Pro^*}(crs, c, \pisnark)$
    %\item If there exists $j\in [n]$ such that $L(G)_j\neq L^*$:
    %\begin{itemize}
        %\item Let $v$ be the node with the $j$th smallest identifier, and let $(c, i_v, \Lambda_{i_v}, \pisnark) = \pi_v$ obtained from $\Pro^*(crs, 1^\lambda, n)$.
        %\item $i = i_v$.
        %\item $\Lambda_i = \Lambda_{i_v}$.
        %\item Let $\aux$ be the auxiliary information obtained from $\VCCom(\crs, L_1^*,\ldots, L_n^*)$.\\ $\Lambda_i' = \VCOpen(crs, L^*_i, i_v, \aux)$.
    %\end{itemize}
    %Otherwise, $i, \Lambda_i, \Lambda'_i = \bot, \bot, \bot$
%\end{itemize}
%Let $w^*$ be the witness obtained from $\SNARKExt_{\Pro^*}$. Let $\var{SNARKBreak}$ be the event that $M$ rejects $(c, 1^\lambda, L^*, w^*)$, and let $\var{VCBreak}$ be the event that $\Adv$ outputs $i, \Lambda_i, \Lambda'_i$ such that
%\begin{gather*}
    %\VCVer(\crs, c, L_i, i, \Lambda_i) = 1 \wedge \VCVer(\crs, c, L_i', i, \Lambda_i') = 1
%\end{gather*}
%
%Whenever the event of \ref{eq:soundnessBreakNP} occurs, one of the following must hold:
%\begin{itemize}
    %\item $M$ rejects $(c, 1^\lambda, L^*, w^*)$, in which case, $\var{SNARKBreak}$ occurs.
    %\item $M$ accepts $(c, 1^\lambda, L^*, w^*$. In that case, since $G\notin \Lan$, $L^*\neq L(G)$. So, $\Adv$ outputs $i, \Lambda_i$ where the node $v$ that satisfies $i_v = i$, simulated $\VCVer(\crs, c, L_i, i, \Lambda_i)$ which outputted 1, since $v$ accepted.
    %In addition, since $M$ accepts $(c, 1^\lambda, L^*, w^*$, we have that $\VCCom(\crs, L_1^*,\ldots, L_n^*) = c, \aux$, and from the completeness of the VC, since $L_i'$ was obtained by $\VCOpen(crs, L^*_i, i_v, \aux)$, we get that $\VCVer(\crs, c, L_i', i, \Lambda_i') = 1$. So, $\var{VCBreak}$ occur. 
%\end{itemize}
%Since the event of \ref{eq:soundnessBreakNP} happens with probability at least $\alpha(\lambda)$, one of the events $\SNARKBreak, \VCBreak$ happens with probability at least $\alpha(\lambda)/2$, which is also a non-negligible function of $\lambda$, and so $\Adv$ breaks at least one of the following: the argument of knowledge property of the SNARK, the position-binding of the VC.
%\end{proof}
%



\subsection{Succinct Distributed Arguments for P from RAM SNARGs}\label{app:dargsForP}
In this section we use Flexible RAM SNARGs to construct a succinct distributed argument for $\PP$. Such RAM SNARGs are defined w.r.t some hash family with local opening. For our use, that hash family will be a succinct vector commitment, which already satisfies all of the hash family with local openings requirements. For our use, we also require that the vector commitment has the following property.\footnote{
A succinct, inverse collision-resistant VC can be instanciated by a Merkle Tree \cite{merkle1989certified}.
}
\begin{definition}[Inverse Collision-Resistance]
A VC $(\Gen, \Com, \Open, \Ver)$ is \emph{Inverse Collision-Resistant} if for any efficient adversary $\calA$, there exists a negligible function $\epsilon(\cdot)$, such that for every $\lambda\in \mathbb{N}$,
\begin{gather*}
    \prob{}
    {
    \begin{array}{ll}
         \forall i: \Ver(crs, C^*, m, i) = 1 \\
         \wedge C^* \neq C
    \end{array}
    \middle\vert
    \begin{array}{ll}
         crs \leftarrow Gen(1^\lambda, q) \\
         C^*, \{(m_i, \lambda_i)\}_{i\in [q]} = \calA(crs) \\
         C \leftarrow \Com(crs, m_1,\ldots,m_q)
    \end{array}
    } \leq \epsilon(\lambda)
\end{gather*}
\end{definition}

\begin{theorem}\label{theo:P}
Let $\Lan$ be a graph language, such that $\Lan\in \PP$. Assuming Flexible RAM SNARGs for $\PP$ and Inverse Collision-Resistant VC exist, there is a succinct distributed argument for $\Lan$.  
\end{theorem}


Let $\Lan$ be a language on graphs that is decidable in polynomial time, given the entire graph as input, and let $M_\Lan$ be the Turing machine that decides it:
$$G\in \Lan \Leftrightarrow M_\Lan(L(G)) = 1$$
Fix a vector commitment $(\VCGen, \VCCom, \VCOpen, \VCVer)$ that is inverse collision-resistant and a RAM SNARG $(\SNARGGen, \SNARGPro, \SNARGVer)$ for $M_\Lan$, corresponding to the vector commitment as the hash with local opening. The succinct distributed argument for $\Lan$, $(\Gen, \Pro, \Ver)$, is defined as follows.
\begin{subfigures}\label{fig:dargsForP}
    \begin{nicefig}[h]{The Setup Procedure of Section~\ref{sec:dargsForP}: $\Gen(1^\secpar, n)$ }{fig:dargsForP_Setup}
        \begin{algorithmic}[1]
            \State Compute: $\crsvc = \VCGen(1^\secpar, n)$
            \State Compute: $\crssnarg = \SNARGGen(1^\secpar, 1^n)$
            \State Output: $\crs = (\crsvc, \crssnarg)$
        \end{algorithmic}
    \end{nicefig}
    
    \begin{nicefig}[h]{The Prover of Section~\ref{sec:dargsForP} : $\Pro(\crs, G)$ }{fig:dargsForP_Prover}
        \begin{algorithmic}[1]
            \State Parse $\crs=(\crsvc, \crssnarg)$
            \State Represent $G$ as an adjacency list $L = L_1,\ldots, L_n$.
            \ForEach{$i\in [n]$}
                \State Set $v\in V(G)$ to be the node with the $i$ smallest identifier.
                \State Set $i_v = i$, $L_{i_v} = (v, N_G(v_i))$            
            \EndFor
            \State Compute $(d,\aux) = \VCCom(\crsvc, L_1,\ldots, L_n)$
            \State Compute for every $i$: $\Lambda_i = \VCOpen(\crsvc, L_i, i, aux)$
            \State Compute $\pisnarg, b = \SNARGPro(\crssnarg, L)$
            \State Output $\{\pi_v\}_{v\in V(G)}$, where for every $v\in V(G)$: $\pi_v = (d, i_v, \Lambda_{i_v}, \pisnarg)$
        \end{algorithmic}
    \end{nicefig}

    \begin{nicefig}[h]{The Verifier of Section~\ref{sec:dargsForP} : $\Ver(\crs, \var{N}, v, \pi)$ }{fig:dargsForP_Verifier}
        \begin{algorithmic}[1]
            \State Parse $\crs=(\crsvc, \crssnarg)$
            \State Parse $\pi = (c, i, \Lambda_i, \pisnarg)$
            \State Verify that $\forall u\in \var{N}$, $d(u) = d$ and $\pisnark(u) = \pisnark$ (otherwise output 0)\label{step:verifyComP}
            \State Output 1 if the following holds:
            \begin{itemize}
                \item $\VCVer(\crsvc, d, (v,\var{N}), i, \Lambda_i) = 1$
                \item $\SNARGVer(\crssnarg, d, \pi) = 1$
            \end{itemize}
        \end{algorithmic}
    \end{nicefig}
\end{subfigures}


We now prove the following statement, from which Theorem~\ref{theo:P} follows.
\begin{claim}
$\Gen, \Pro, \Ver$ is a succinct distributed argument.
\end{claim}

\begin{proof}%\label{step:soundnessBreakP}
    Completeness and succinctness follow immediately from the completeness and the succinctness of the VC and the SNARG. We proceed to the proof of soundness.
    Assume towards contradiction that there exists an efficient prover $\Pro^*$ and a non-negligible function $\alpha(\cdot)$ such that:
    \begin{gather}\label{eq:soundnessBreakP}
        \prob{}
        {
        \begin{array}{ll}
        G\notin \Lan \\
        \wedge\ \forall v\in V(G) : \Ver(\var{crs}, \var{N}_G(v), v, \pi_v) = 1
        \end{array}
        \middle\vert
        \begin{array}{ll}
        \var{crs} \leftarrow \Gen(1^\lambda, n) \\
        G, \{\pi_{v}\}_{v\in V(G)} \leftarrow \Pro^*(\var{crs}, 1^\lambda, 1^n) \\
        \end{array}
        } \geq \alpha(\lambda)
    \end{gather}

    First, note that since all nodes verify the consistency of $d$ and $\pisnarg$ with their neighbors,(in Step~\ref{step:verifyComP}), if all nodes accept, then the prover gave the same commitment (digest), and the same SNARG proof $\pi_{\SNARG}$ to all of the nodes.
    
    We use $\Pro^*$ to construct an efficient adversary $\Adv$ that breaks either one of the properties of the SNARG, or one of the properties of the VC. Let $G, \{\pi_{v}\}_{v\in V(G)}$ be the graph and the proofs outputted from $\Pro^*(\crs, 1^\lambda ,n)$, and let $\crsvc, \crssnarg$ be the parsed reference strings from $\crs$.
    On input $\crs, 1^\lambda, n$, $\Adv$ outputs $d, d^*, \pi_0, \pi_1$, where:
    \begin{itemize}
        \item $d^*$ is the commitment in $\pi_v$ that is consistent across all $v\in V(G)$.
        \item $d, \aux = \VCCom(\crsvc, L(G))$. ($\Adv$ doesn't output $\aux$ but we refer it later)
        \item $\pi_0 = \SNARGPro(\crssnarg, L(G))$.
    \end{itemize}
    For every $v\in V(G)$, let $\Lambda_{i_v} = \VCOpen(\crsvc, L_{i_v}, i_v, \aux)$.
    We define the following events.
    \begin{itemize}
        \item Let $\VCCompBreak$ be the event that $d\neq d^*$ and there exists some $v\in V(G)$ such that $\VCVer$ rejects $(\crsvc, d, (v,N_G(v)), i \Lambda_{i_v})$.
        \item Let $\ICRBreak$ be the event that $d\neq d^*$ $\VCVer$ accepts $(\crsvc, d, (v,N_G(v)), i \lambda_{i_v})$.
        \item Let $\SNARGComBreak$ be the event that $\SNARGVer$ rejects $(\crssnarg, d, 0, \pi_0)$.
        \item Let $\SNARGSoundBreak$ be the event that $d = d^*$ and $\SNARGVer$ accepts $(\crssnarg, d, 1, \pi_1)$ and $(\crssnarg, d, 0, \pi_0)$.
    \end{itemize}
    Whenever the event in \ref{eq:soundnessBreakP} occurs, one of the following must hold:
    \begin{itemize}
        \item $d \neq d^*$, so either $\VCCompBreak$ or $\ICRBreak$ occur.
        \item $d = d^*$, so either $\SNARGComBreak$ or $\SNARGSoundBreak$ occur.
    \end{itemize}
    Since the event in \ref{eq:soundnessBreakP} happens with probability at least $\alpha(\lambda)$, one of the events $\VCCompBreak$, $\ICRBreak$, $\SNARGSoundBreak$, $\SNARGCompBreak$ happens with probability at least $\alpha(\lambda)/4$, which is also a non-negligible function of $\lambda$, and so, $\Adv$ breaks at least one of the following: the VC's completeness property, the VC's position-binding property, the SNARG's completeness property, the SNARG's soundness property.
Completeness, succinctness, and verifier efficiency follow naturally from the primitives' properties.
\end{proof}

\subsection{Certifying Executions of Computationally-Efficient Distributed Algorithms}
\label{app:distprover}

In the general case where we have inputs $x : V(G) \rightarrow \calX$
and outputs $y : V(G) \rightarrow \calY$,
the consistency of the local computation at a specific node is captured
by the
language $\calD$,
which consists of all triplets $(\hk, I(v), W(v))$ such that:
\begin{itemize}
	\item $\hk$ is a hash key obtained by calling $\SCRHGen$,
	\item $I(v) = (v, x(v), N(v), y(v), s_{\var{in}}(v), s_{\var{out}}(v))$,
		where
		$v \in \calU$ is the UID of a node,
		$x(v) \in \calX$ is the input of the node,
		$N(v) \in \calU^{\ast}$ is the neighborhood of the node,
		$y(v) \in \calY$ is an output value, and $s_{\var{in}}(v), s_{\var{out}}(v)$ are hash sums;
	\item $W(v) = (\var{msgout}(v), \var{msgin}(v))$ consists of two sets of messages;
	\item $(\hk, I(v), W(v)) \in \calD$ iff when the distributed algorithm $D$ is executed 
at a node with UID $v$, input $x(v)$ and neighbors $N(v)$,
and the incoming messages at node $v$ are $\var{msgin}(v)$,
the node produces output $y(v)$ and sends the messages $\var{msgout}(v)$,
and furthermore,
\begin{equation}
	s_{\var{in}} = \sum_{\var{msg} \in \var{msgin}} \SCRHHash(\hk, \var{msg}),
	\qquad
	s_{\var{out}} = \sum_{\var{msg} \in \var{msgout}} \SCRHHash(\hk, \var{msg}).
	\label{cond:msg_hash}
\end{equation}
\end{itemize}


Let $G = (V, E)$ be a graph of size $n$, and let $\ell = \poly(n)$ be the maximum encoding length of
a message sent by $D$ in graphs of size $n$.%
\footnote{Recall that the encoding of a message consists
of the round number, the edge on which it is sent, and the message contents;
for an algorithm that runs on polynomial rounds and sends polynomially-long messages,
the encoding of a message is polynomial in $n$.}


 \begin{subfigures}\label{fig:distprover}

	 \begin{nicefig}[h]{The Setup Procedure of Section~\ref{sec:distprover}: $\Gen(1^\secpar, n)$}{fig:distprover_Setup}
\begin{algorithmic}[1]
	\State $\hk \leftarrow \SCRHGen(1^\secpar, \ell(n))$
	\State $\crssnark \leftarrow \SNARKGen(1^\secpar, n')$
	\Comment{$n'$ is the encoding length of $(\hk, I(v))$ for a single vertex $v$ in graphs of size $n$}
	\State output $(\hk, \crssnark)$
\end{algorithmic}
\end{nicefig}

	 \begin{nicefig}[h]{The Distributed Prover of Section~\ref{sec:distprover}: $\Pro(\crs, (G, x, y))$}{fig:distprover_Prover}
		 The prover is the following distributed algorithm, executed jointly by the nodes of $G$.
\begin{algorithmic}[1]
	\State Parse $\crs = (\hk, \crssnark)$
	\Comment{All nodes must know the CRS}
	\State Compute the messages $\set{ \var{msgout}(v), \var{msgin}(v) }_{v \in V(G)}$ sent and received during
	the execution of $D$ at each $v \in V(G)$ \Comment{This can be done while $D$ is executing}
	%\State $\hk \leftarrow \SCRHGen( 1^{\secpar}, \ell )$
	\ForEach{$v \in V(G)$} \Comment{In parallel}
		\State $s_{\var{out}}(v) \leftarrow \sum_{\var{msg} \in \var{msgout}(v) } \SCRHHash(\hk, \var{msg})$
		\State $s_{\var{in}}(v) \leftarrow \sum_{\var{msg} \in \var{msgin}(v) } \SCRHHash(\hk, \var{msg})$
		\State $I(v) \leftarrow (v, x(v), N_G(v), y(v), s_{\var{in}}(v), s_{\var{out}}(v))$
		\State $W(v) \leftarrow ( \var{msgout}(v), \var{msgin}(v) )$
		\State $\pisnark(v) \leftarrow \Pro(\var{crs}, (\hk, I(v)), W(v))$
	\EndFor
	\State Compute a spanning tree $T$ of $G$, of height $\leq 2 \mathrm{diam}(G)$
	\State $r \leftarrow $ the root of $T$
	\State $d(r) \leftarrow 0, p(r) \leftarrow \bot$
	\State By broadcast down the tree, for each node $v$ with parent $u$,
		set $p(v) \leftarrow u, d(v) \leftarrow d(u) + 1$
	\State By convergecast up the tree, for each node $v$ set 
	$S_{\var{out}}(v) \leftarrow s_{\var{out}}(v) + \sum_{u \in \mathrm{children}(v)} S_{\var{out}}(u)$,
	$S_{\var{in}}(v) \leftarrow s_{\var{in}}(v) + \sum_{u \in \mathrm{children}(v)} S_{\var{in}}(u)$
	\State Output $\pi(v) = (p(v), d(v), r, s_{\var{out}}(v), s_{\var{in}}(v), S_{\var{out}}(v), S_{\var{in}}(v), \pisnark(v))$
	at each node $v$
\end{algorithmic}
\end{nicefig}

\begin{nicefig}[h]{The Verifier of Section~\ref{sec:distprover} }{fig:distprover_Verifier}
	The verifier at node $v$, with setup $\crs = (\hk, \crssnark)$
	and
	certificate 
	\begin{equation*}
	\pi(v) = (p(v), d(v), r(v), s_{\var{out}}(v), s_{\var{in}}(v), S_{\var{out}}(v), S_{\var{in}}(v), \pisnark(v)),
\end{equation*}
verifies the following conditions:
\begin{algorithmic}[1]
	\State $r(v) = r(u)$ for every $u \in N(v)$
	\If{$r(v) = v$}
		\State $d(v) = 0$
		\State $S_{\var{out}}(v) = S_{\var{in}}(v)$
	\Else
		\State $p(v) \in N(v)$
		\State $d(v) = d(p(v)) + 1$
	\EndIf
	\State $S_{\var{out}}(v) = s_{\var{out}}(v) + \sum_{u \in N(v) : p(u) = v} S_{\var{out}}(u)$
	\State $S_{\var{in}}(v) = s_{\var{in}}(v) + \sum_{u \in N(v) : p(u) = v} S_{\var{in}}(u)$
	\State $\SNARKVer(\crssnark, (\hk, (v, x(v), N(v), y(v), s_{\var{in}}(v), s_{\var{out}}(v))), \pisnark(v)) = 1$.
\end{algorithmic}
\end{nicefig}

\end{subfigures}


\begin{claim}
    $\Gen, \Pro, \Ver$ is a succinct distributed argument for $\Lan_D$
\end{claim}

\begin{proof}
Suppose for the sake of contradiction that there is an efficient adversary $\Adv$ such that for some
non-negligible function $\alpha(\cdot)$
and for all sufficiently large $n$,
we have
\begin{gather*}
    \prb
    {
    G\notin \Lan \\
    \wedge\ \forall v\in V(G) : \\
    \Ver(\var{\crs}, v, (x(v), y(v)),\\
    \qquad\qquad N(v), \pi(v), \pi(N(v))) = 1
    }
    {
    \var{\crs} \leftarrow \Gen(1^\secpar, n) \\
    (G, \set{\pi(v) }_{v\in V(G)}) \leftarrow \Pro^*(\var{\crs}, 1^\secpar, 1^n)
    } \geq \alpha(\secpar).
\end{gather*}
%\TODO{$\Lan$ or $\Lan_D$?}
We use $\Adv^*$ to construct an efficient adversary $\Adv'$ that breaks either the SNARK or the hash.
$\Adv'$ works as follows:
	After obtaining ${\crs} \leftarrow \Gen(1^\secpar, n)$,
	where $\crs = (\hk, \crssnark)$,
	we execute 
	$\Adv^*$ to obtain a graph $G$, along with certificates $\set{ \pi(v) }_{v \in V(G)}$
	for the nodes of $V$.
	From the certificates, $\Adv'$ extracts:
	\begin{itemize}
		\item A collection of hash-sums $\set{ s_{\var{in}}(v), s_{\var{out}}(v) }_{v \in V}$.

			As usual, let us denote $I(v) = (v, x(v), N(v), y(v), s_{\var{in}}(v), s_{\var{out}}(v))$
			for each $v \in V(G)$.
		\item A collection of SNARK proofs $\set{ \pisnark(v) }_{v \in V}$.
		\item 
			Recall that the SNARK proof $\pisnark(v)$ is for the statement 
			``$\exists W(v) : (\hk, I(v), W(v)) \in \calD$''.
			Using the extraction algorithm
			$\SNARKExt{\Adv^*}(\crssnark, (\hk, I(v))$ of the SNARK,
			we extract a witness for each node $v$
			in the form of message sets,
			$W(v) = (\var{msgin}(v), \var{msgout}(v))$.
	\end{itemize}


	Let $\var{ST}$ be the event that the verificiation of the spanning tree
			distances, root and partial sums succeeds at all nodes.
			An easy induction on the height of the tree
			shows that when this event occurs
			we have
			\begin{equation}
				S_{\var{in}}(r) =
				\sum_{v \in V} s_{\var{in}}(v),
				\qquad
				S_{\var{out}}(r) = \sum_{v \in V} s_{\var{out}}(v),
				\label{cond:partial_sums}
			\end{equation}
		where $r \in V$ is the the unique node specified in all the certificates
		as the root of the spanning tree.
		From now on, we condition on this event,
		and refer to $r$ as ``the root''.
		Recall also that as part of the partial sum verification,
		the root verifies that
		\begin{equation}
			S_{\var{in}}(r) = S_{\var{out}}(r).
			\label{cond:sums_equal}
		\end{equation}


	We claim that if $G \not \in \calL$,
	whenever all nodes accept, one of the following events must occur:
	\begin{itemize}
		\item $\var{HashCheat}$:
			the two collections of messages
			given by $\var{Out} = \set{ \var{msg} \in \var{msgout}(v) : v \in V }$
			and by $\var{In} = \set{ \var{msg} \in \var{msgin}(v) : v \in V}$
			are not equal, but they
			break the SCRH property by having
			$\sum_{ m \in \var{Out} }\SCRHHash(\hk, m) = \sum_{ m' \in \var{In} }\SCRHHash(\hk, m')$;
			or
		\item $\var{SnarkCheat}$:
			at some node $v$ we have
			%the prover has found a proof $\pi_{SNARK}(v)$ and a witness $W_v$
			%such that
			$\SNARKVer(\crssnark, (\hk, I(v)), \pisnark(v)) = 1$
			but $M$ rejects $(\hk, I(v), W(v))$,
			violating the argument of knowledge property of the SNARK.
	\end{itemize}
	To see this, suppose that all nodes accept,
	but neither event occurs. We divide into cases:
	\begin{itemize}
		\item Suppose that $M$ accepts $(\hk, I(v), W(v))$ at all $v \in V(G)$
			(i.e., $\var{SnarkCheat}$ has not occurred),
			and the message collections $\var{Out}$ and $\var{In}$ are not equal.
			Recall that $M$ verifies~\eqref{cond:msg_hash},
			and that the event $\var{ST}$ on which we are conditioning 
			implies~\eqref{cond:partial_sums},~\eqref{cond:sums_equal}.
			But~\eqref{cond:msg_hash} and~\eqref{cond:partial_sums}
			together imply that
			\begin{equation*}
				S_{\var{in}}(r) = \sum_{m \in \var{In}} \SCRHHash(\hk, m),
				\qquad
				S_{\var{out}}(r) = \sum_{m \in \var{Out}} \SCRHHash(\hk, m),
			\end{equation*}
			and~\eqref{cond:sums_equal} implies that
			the two hash-sums are equal, $S_{\var{in}}(r) = S_{\var{out}}(r)$;
			thus, $\var{HashCheat}$ has occurred.
		\item Now suppose that the message collections $\var{Out}$ and $\var{In}$ \emph{are} equal,
			but $G \notin \calL_D$.
			Then there is some node $v \in V$ such that $y(v)$ is not the output of the
			distributed algorithm $D$ at node $v$ when executed in $G$.


			Let us say that a message $\var{msg} = (r, \set{u,w}, m)$
			is \emph{correct} if it would be sent in the execution of $D$ in $G$.
			There are two cases:
			\begin{itemize}
				\item All messages in $W(v) = (\var{msgin}(v), \var{msgout}(v))$
					are correct.
					In this case, $M( \hk, I(v), W(v))$
					rejects, as it is not true that $v$
					produces the output $y(v)$ (which appears in $I(v)$)
					when $D$ is executed in $G$.
				\item Some message in $W(v) = (\var{msgin}(v), \var{msgout}(v))$
					is not correct.
					In this case,
				let $t$ be the first round such that $\var{In} = \var{Out}$ 
			includes some incorrect round-$t$ message $\var{msg} = (t, \set{u,v}, m)$,
			and let $u$ be the node such that $\var{msg} \in \var{msgout}(u)$.
			Then $M$ rejects $(\hk, I(u), W(u))$:
			at round $t$, if fed the messages in $\var{msgin}(u)$ up to round $t - 1$
			(which are all correct),
			it is not true that $u$ sends $\var{msg}$ (as this message is incorrect).
			\end{itemize}
			In both cases, $M$ rejects $(\hk, I(u), W(u))$,
			but we assumed that all nodes accept,
			and therefore $\var{SnarkCheat}(v)$ has occurred.
	\end{itemize}
 %\Enote{I think the second one should be first, then it's cleaner to suppose what we suppose}
	We conclude that one of the events $\var{HashCheat},\var{SnarkCheat}$
	occurs with probability at least $\alpha(\lambda)/2$,
	as $\Adv$ generates $G \not \in \calL$ and certificates that all nodes accept
	with probability at least $\alpha(\secpar)$,
	and whenever this occurs, at least one of the events 
 	$\var{HashCheat}, \var{SnarkCheat}$ occurs.

	Since $\alpha(\cdot)$ is non-negligible, $\alpha(\cdot) / 2$
	is also non-negligible.
	If $\var{HashCheat}$ occurs with probability at least $\alpha(\secpar) / 2$,
	this violates the SCRH property of the hash function;
	and if $\var{SnarkCheat}$ occurs with probability at least $\alpha(\secpar) / 2$,
	this violates the argument of knowledge property of the SNARK.
\end{proof}

